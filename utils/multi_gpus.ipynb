{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多GPU训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.01\n",
    "W1 = torch.randn(size=(20, 1, 3, 3)) * scale\n",
    "b1 = torch.zeros(20)\n",
    "W2 = torch.randn(size=(50, 20, 5, 5)) * scale\n",
    "b2 = torch.zeros(50)\n",
    "W3 = torch.randn(size=(800, 128)) * scale\n",
    "b3 = torch.zeros(128)\n",
    "W4 = torch.randn(size=(128, 10)) * scale\n",
    "b4 = torch.zeros(10)\n",
    "params = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "\n",
    "def lenet(X, params):\n",
    "    h1_conv = F.conv2d(input=X, weight=params[0], bias=params[1])\n",
    "    h1_activation = F.relu(h1_conv)\n",
    "    h1 = F.avg_pool2d(input=h1_activation, kernel_size=(2, 2), stride=(2, 2))\n",
    "    h2_conv = F.conv2d(input=h1, weight=params[2], bias=params[3])\n",
    "    h2_activation = F.relu(h2_conv)\n",
    "    h2 = F.avg_pool2d(input=h2_activation, kernel_size=(2, 2), stride=(2, 2))\n",
    "    h2 = h2.reshape(h2.shape[0], -1)\n",
    "    h3_linear = torch.mm(h2, params[4]) + params[5]\n",
    "    h3 = F.relu(h3_linear)\n",
    "    y_hat = torch.mm(h3, params[6]) + params[7]\n",
    "    return y_hat\n",
    "\n",
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向多个设备分发参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1 weight: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "b1 grad: None\n"
     ]
    }
   ],
   "source": [
    "def get_params(params, device):\n",
    "    new_params = [p.clone().to(device) for p in params]\n",
    "    for p in new_params:\n",
    "        p.requires_grad_()\n",
    "    return new_params\n",
    "\n",
    "new_params = get_params(params, d2l.try_gpu(0))\n",
    "print('b1 weight:', new_params[1])\n",
    "print('b1 grad:', new_params[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### allreduce 函数将所有向量相加，并将结果广播给所有 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before allreduce:\n",
      " tensor([[1., 1.]], device='cuda:0') \n",
      " tensor([[2., 2.]])\n",
      "after allreduce:\n",
      " tensor([[3., 3.]], device='cuda:0') \n",
      " tensor([[3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "def allreduce(data):\n",
    "    for i in range(1, len(data)):\n",
    "        data[0][:] += data[i].to(data[0].device)\n",
    "    for i in range(1, len(data)):\n",
    "        data[i] = data[0].to(data[i].device)\n",
    "\n",
    "data = [torch.ones((1, 2), device=d2l.try_gpu(i)) * (i + 1) for i in range(2)]\n",
    "print('before allreduce:\\n', data[0], '\\n', data[1])\n",
    "allreduce(data)\n",
    "print('after allreduce:\\n', data[0], '\\n', data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将一个小批量数据均匀地分布在多个 GPU 上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected a non cpu device, but got: cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Workspace\\awesome-deep-learning\\utils\\multi_gpus.ipynb 单元格 10\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# devices = [torch.device('cuda:0'), torch.device('cuda:1')]\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m devices \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m), torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m split \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mparallel\u001b[39m.\u001b[39;49mscatter(data, devices)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minput :\u001b[39m\u001b[39m'\u001b[39m, data)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mload into\u001b[39m\u001b[39m'\u001b[39m, devices)\n",
      "File \u001b[1;32mc:\\Users\\fdse\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\parallel\\scatter_gather.py:44\u001b[0m, in \u001b[0;36mscatter\u001b[1;34m(inputs, target_gpus, dim)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39m# After scatter_map is called, a scatter_map cell will exist. This cell\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39m# has a reference to the actual function scatter_map, which has references\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m# to a closure that has a reference to the scatter_map cell (because the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m# fn is recursive). To avoid this reference cycle, we set the function to\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39m# None, clearing the cell\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m     res \u001b[39m=\u001b[39m scatter_map(inputs)\n\u001b[0;32m     45\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     scatter_map \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fdse\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\parallel\\scatter_gather.py:27\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscatter_map\u001b[39m(obj):\n\u001b[0;32m     26\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m Scatter\u001b[39m.\u001b[39;49mapply(target_gpus, \u001b[39mNone\u001b[39;49;00m, dim, obj)\n\u001b[0;32m     28\u001b[0m     \u001b[39mif\u001b[39;00m _is_namedtuple(obj):\n\u001b[0;32m     29\u001b[0m         \u001b[39mreturn\u001b[39;00m [\u001b[39mtype\u001b[39m(obj)(\u001b[39m*\u001b[39margs) \u001b[39mfor\u001b[39;00m args \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(scatter_map, obj))]\n",
      "File \u001b[1;32mc:\\Users\\fdse\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\autograd\\function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mapply(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[0;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fdse\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:89\u001b[0m, in \u001b[0;36mScatter.forward\u001b[1;34m(ctx, target_gpus, chunk_sizes, dim, input)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(ctx, target_gpus, chunk_sizes, dim, \u001b[39minput\u001b[39m):\n\u001b[1;32m---> 89\u001b[0m     target_gpus \u001b[39m=\u001b[39m [_get_device_index(x, \u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m target_gpus]\n\u001b[0;32m     90\u001b[0m     ctx\u001b[39m.\u001b[39mdim \u001b[39m=\u001b[39m dim\n\u001b[0;32m     91\u001b[0m     ctx\u001b[39m.\u001b[39minput_device \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mget_device() \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\fdse\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:89\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(ctx, target_gpus, chunk_sizes, dim, \u001b[39minput\u001b[39m):\n\u001b[1;32m---> 89\u001b[0m     target_gpus \u001b[39m=\u001b[39m [_get_device_index(x, \u001b[39mTrue\u001b[39;49;00m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m target_gpus]\n\u001b[0;32m     90\u001b[0m     ctx\u001b[39m.\u001b[39mdim \u001b[39m=\u001b[39m dim\n\u001b[0;32m     91\u001b[0m     ctx\u001b[39m.\u001b[39minput_device \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mget_device() \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\fdse\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\_utils.py:715\u001b[0m, in \u001b[0;36m_get_device_index\u001b[1;34m(device, optional, allow_cpu)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, torch\u001b[39m.\u001b[39mdevice):\n\u001b[0;32m    714\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_cpu \u001b[39mand\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 715\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected a non cpu device, but got: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(device))\n\u001b[0;32m    716\u001b[0m     device_idx \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m device\u001b[39m.\u001b[39mindex\n\u001b[0;32m    717\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: Expected a non cpu device, but got: cpu"
     ]
    }
   ],
   "source": [
    "data = torch.arange(20).reshape(4, 5)\n",
    "devices = [torch.device('cuda:0'), torch.device('cuda:1')]\n",
    "split = nn.parallel.scatter(data, devices)\n",
    "print('input :', data)\n",
    "print('load into', devices)\n",
    "print('output:', split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(X, y, devices):\n",
    "    \"\"\"将`X`和`y`拆分到多个设备上\"\"\"\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    return (nn.parallel.scatter(X, devices), nn.parallel.scatter(y, devices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在一个小批量上实现多 GPU 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(X, y, device_params, devices, lr):\n",
    "    X_shards, y_shards = split_batch(X, y, devices)\n",
    "    ls = [\n",
    "        loss(lenet(X_shard, device_W),\n",
    "             y_shard).sum() for X_shard, y_shard, device_W in zip(\n",
    "                 X_shards, y_shards, device_params)]\n",
    "    for l in ls:\n",
    "        l.backward()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(device_params[0])):\n",
    "            allreduce([device_params[c][i].grad for c in range(len(devices))])\n",
    "    for param in device_params:\n",
    "        d2l.sgd(param, lr, X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "    device_params = [get_params(params, d) for d in devices]\n",
    "    num_epochs = 10\n",
    "    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "    timer = d2l.Timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        timer.start()\n",
    "        for X, y in train_iter:\n",
    "            train_batch(X, y, device_params, devices, lr)\n",
    "            torch.cuda.synchronize()\n",
    "        timer.stop()\n",
    "        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(\n",
    "            lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))\n",
    "    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n",
    "          f'on {str(devices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在单个GPU上运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_gpus=1, batch_size=256, lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 增加为2个GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_gpus=2, batch_size=256, lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多GPU的简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fdse\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Residual.__init__() got multiple values for argument 'use_1x1conv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Workspace\\awesome-deep-learning\\utils\\multi_gpus.ipynb 单元格 23\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     net\u001b[39m.\u001b[39madd_module(\u001b[39m\"\u001b[39m\u001b[39mfc\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m                    nn\u001b[39m.\u001b[39mSequential(nn\u001b[39m.\u001b[39mFlatten(), nn\u001b[39m.\u001b[39mLinear(\u001b[39m512\u001b[39m, num_classes)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m net\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m net \u001b[39m=\u001b[39m resnet18(\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m devices \u001b[39m=\u001b[39m d2l\u001b[39m.\u001b[39mtry_all_gpus()\n",
      "\u001b[1;32md:\\Workspace\\awesome-deep-learning\\utils\\multi_gpus.ipynb 单元格 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m net \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     nn\u001b[39m.\u001b[39mConv2d(in_channels, \u001b[39m64\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     nn\u001b[39m.\u001b[39mBatchNorm2d(\u001b[39m64\u001b[39m), nn\u001b[39m.\u001b[39mReLU())\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m net\u001b[39m.\u001b[39madd_module(\u001b[39m\"\u001b[39m\u001b[39mresnet_block1\u001b[39m\u001b[39m\"\u001b[39m, resnet_block(\u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m2\u001b[39m, first_block\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m net\u001b[39m.\u001b[39madd_module(\u001b[39m\"\u001b[39m\u001b[39mresnet_block2\u001b[39m\u001b[39m\"\u001b[39m, resnet_block(\u001b[39m64\u001b[39;49m, \u001b[39m128\u001b[39;49m, \u001b[39m2\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m net\u001b[39m.\u001b[39madd_module(\u001b[39m\"\u001b[39m\u001b[39mresnet_block3\u001b[39m\u001b[39m\"\u001b[39m, resnet_block(\u001b[39m128\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m net\u001b[39m.\u001b[39madd_module(\u001b[39m\"\u001b[39m\u001b[39mresnet_block4\u001b[39m\u001b[39m\"\u001b[39m, resnet_block(\u001b[39m256\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m2\u001b[39m))\n",
      "\u001b[1;32md:\\Workspace\\awesome-deep-learning\\utils\\multi_gpus.ipynb 单元格 23\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_residuals):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m first_block:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         blk\u001b[39m.\u001b[39mappend(\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m             d2l\u001b[39m.\u001b[39;49mResidual(in_channels, out_channels, use_1x1conv\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                          strides\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/awesome-deep-learning/utils/multi_gpus.ipynb#X31sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         blk\u001b[39m.\u001b[39mappend(d2l\u001b[39m.\u001b[39mResidual(out_channels, out_channels))\n",
      "\u001b[1;31mTypeError\u001b[0m: Residual.__init__() got multiple values for argument 'use_1x1conv'"
     ]
    }
   ],
   "source": [
    "def resnet18(num_classes, in_channels=1):\n",
    "    \"\"\"稍加修改的 ResNet-18 模型。\"\"\"\n",
    "    def resnet_block(in_channels, out_channels, num_residuals,\n",
    "                     first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(\n",
    "                    d2l.Residual(in_channels, out_channels, use_1x1conv=True,\n",
    "                                 strides=2))\n",
    "            else:\n",
    "                blk.append(d2l.Residual(out_channels, out_channels))\n",
    "        return nn.Sequential(*blk)\n",
    "\n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(64), nn.ReLU())\n",
    "    net.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\n",
    "    net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\n",
    "    net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\n",
    "    net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\n",
    "    net.add_module(\"global_avg_pool\", nn.AdaptiveAvgPool2d((1, 1)))\n",
    "    net.add_module(\"fc\",\n",
    "                   nn.Sequential(nn.Flatten(), nn.Linear(512, num_classes)))\n",
    "    return net\n",
    "\n",
    "net = resnet18(10)\n",
    "devices = d2l.try_all_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "    def init_weights(m):\n",
    "        if type(m) in [nn.Linear, nn.Conv2d]:\n",
    "            nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "    net.apply(init_weights)\n",
    "    net = nn.DataParallel(net, device_ids=devices)\n",
    "    trainer = torch.optim.SGD(net.parameters(), lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    timer, num_epochs = d2l.Timer(), 10\n",
    "    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        timer.start()\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(devices[0]), y.to(devices[0])\n",
    "            l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "        timer.stop()\n",
    "        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),))\n",
    "    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n",
    "          f'on {str(devices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在单个GPU上训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, num_gpus=1, batch_size=256, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 2 个 GPU 进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, num_gpus=2, batch_size=512, lr=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
