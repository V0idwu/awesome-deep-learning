{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\r\n",
    "\r\n",
    "import sys\r\n",
    "import logging\r\n",
    "import itertools\r\n",
    "import copy\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "np.random.seed(0)\r\n",
    "import pandas as pd\r\n",
    "import gym\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "torch.manual_seed(0)\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "\r\n",
    "logging.basicConfig(level=logging.DEBUG,\r\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\r\n",
    "        stream=sys.stdout, datefmt='%H:%M:%S')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env = gym.make('Pendulum-v0')\r\n",
    "env.seed(0)\r\n",
    "for key in vars(env):\r\n",
    "    logging.info('%s: %s', key, vars(env)[key])\r\n",
    "for key in vars(env.spec):\r\n",
    "    logging.info('%s: %s', key, vars(env.spec)[key])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DQNReplayer:\r\n",
    "    def __init__(self, capacity):\r\n",
    "        self.memory = pd.DataFrame(index=range(capacity),\r\n",
    "                columns=['observation', 'action', 'reward',\r\n",
    "                'next_observation', 'done'])\r\n",
    "        self.i = 0\r\n",
    "        self.count = 0\r\n",
    "        self.capacity = capacity\r\n",
    "\r\n",
    "    def store(self, *args):\r\n",
    "        self.memory.loc[self.i] = args\r\n",
    "        self.i = (self.i + 1) % self.capacity\r\n",
    "        self.count = min(self.count + 1, self.capacity)\r\n",
    "\r\n",
    "    def sample(self, size):\r\n",
    "        indices = np.random.choice(self.count, size=size)\r\n",
    "        return (np.stack(self.memory.loc[indices, field]) for field in\r\n",
    "                self.memory.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class OrnsteinUhlenbeckProcess:\r\n",
    "    def __init__(self, x0):\r\n",
    "        self.x = x0\r\n",
    "\r\n",
    "    def __call__(self, mu=0., sigma=1., theta=.15, dt=.01):\r\n",
    "        n = np.random.normal(size=self.x.shape)\r\n",
    "        self.x += (theta * (mu - self.x) * dt + sigma * np.sqrt(dt) * n)\r\n",
    "        return self.x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DDPGAgent:\r\n",
    "    def __init__(self, env):\r\n",
    "        state_dim = env.observation_space.shape[0]\r\n",
    "        self.action_dim = env.action_space.shape[0]\r\n",
    "        self.action_low = env.action_space.low[0]\r\n",
    "        self.action_high = env.action_space.high[0]\r\n",
    "        self.gamma = 0.99\r\n",
    "\r\n",
    "        self.replayer = DQNReplayer(20000)\r\n",
    "\r\n",
    "        self.actor_evaluate_net = self.build_net(input_size=state_dim, hidden_sizes=[32, 64], output_size=self.action_dim)\r\n",
    "        self.actor_optimizer = optim.Adam(self.actor_evaluate_net.parameters(), lr=0.0001)\r\n",
    "        self.actor_target_net = copy.deepcopy(self.actor_evaluate_net)\r\n",
    "\r\n",
    "        self.critic_evaluate_net = self.build_net(input_size=state_dim + self.action_dim, hidden_sizes=[64, 128])\r\n",
    "        self.critic_optimizer = optim.Adam(self.critic_evaluate_net.parameters(), lr=0.001)\r\n",
    "        self.critic_loss = nn.MSELoss()\r\n",
    "        self.critic_target_net = copy.deepcopy(self.critic_evaluate_net)\r\n",
    "\r\n",
    "    def build_net(self, input_size, hidden_sizes, output_size=1, output_activator=None):\r\n",
    "        layers = []\r\n",
    "        for input_size, output_size in zip(\r\n",
    "            [\r\n",
    "                input_size,\r\n",
    "            ]\r\n",
    "            + hidden_sizes,\r\n",
    "            hidden_sizes\r\n",
    "            + [\r\n",
    "                output_size,\r\n",
    "            ],\r\n",
    "        ):\r\n",
    "            layers.append(nn.Linear(input_size, output_size))\r\n",
    "            layers.append(nn.ReLU())\r\n",
    "        layers = layers[:-1]\r\n",
    "        if output_activator:\r\n",
    "            layers.append(output_activator)\r\n",
    "        net = nn.Sequential(*layers)\r\n",
    "        return net\r\n",
    "\r\n",
    "    def reset(self, mode=None):\r\n",
    "        self.mode = mode\r\n",
    "        if self.mode == \"train\":\r\n",
    "            self.trajectory = []\r\n",
    "            self.noise = OrnsteinUhlenbeckProcess(np.zeros((self.action_dim,)))\r\n",
    "\r\n",
    "    def step(self, observation, reward, done):\r\n",
    "        if self.mode == \"train\" and self.replayer.count < 3000:\r\n",
    "            action = np.random.uniform(self.action_low, self.action_high)\r\n",
    "        else:\r\n",
    "            state_tensor = torch.as_tensor(observation, dtype=torch.float).reshape(1, -1)\r\n",
    "            action_tensor = self.actor_evaluate_net(state_tensor)\r\n",
    "            action = action_tensor.detach().numpy()[0]\r\n",
    "        if self.mode == \"train\":\r\n",
    "            noise = self.noise(sigma=0.1)\r\n",
    "            action = (action + noise).clip(self.action_low, self.action_high)\r\n",
    "\r\n",
    "            self.trajectory += [observation, reward, done, action]\r\n",
    "            if len(self.trajectory) >= 8:\r\n",
    "                state, _, _, act, next_state, reward, done, _ = self.trajectory[-8:]\r\n",
    "                self.replayer.store(state, act, reward, next_state, done)\r\n",
    "\r\n",
    "            if self.replayer.count >= 3000:\r\n",
    "                self.learn()\r\n",
    "        return action\r\n",
    "\r\n",
    "    def close(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    # 用evaluate网络更新target网络\r\n",
    "    def update_net(self, target_net, evaluate_net, learning_rate=0.005):\r\n",
    "        for target_param, evaluate_param in zip(target_net.parameters(), evaluate_net.parameters()):\r\n",
    "            target_param.data.copy_(learning_rate * evaluate_param.data + (1 - learning_rate) * target_param.data)\r\n",
    "\r\n",
    "    def learn(self):\r\n",
    "        # replay\r\n",
    "        states, actions, rewards, next_states, dones = self.replayer.sample(64)\r\n",
    "        state_tensor = torch.as_tensor(states, dtype=torch.float)\r\n",
    "        action_tensor = torch.as_tensor(actions, dtype=torch.long)\r\n",
    "        reward_tensor = torch.as_tensor(rewards, dtype=torch.float)\r\n",
    "        next_state_tensor = torch.as_tensor(next_states, dtype=torch.float)\r\n",
    "        done_tensor = torch.as_tensor(dones, dtype=torch.float)\r\n",
    "\r\n",
    "        # learn critic\r\n",
    "        next_action_tensor = self.actor_target_net(next_state_tensor)\r\n",
    "        noise_tensor = 0.2 * torch.randn_like(action_tensor, dtype=torch.float)\r\n",
    "        noisy_next_action_tensor = (next_action_tensor + noise_tensor).clamp(self.action_low, self.action_high)  # 连续动作\r\n",
    "        next_state_action_tensor = torch.cat([next_state_tensor, noisy_next_action_tensor], 1)\r\n",
    "        next_q_tensor = self.critic_target_net(next_state_action_tensor).squeeze(1)\r\n",
    "        critic_target_tensor = reward_tensor + (1.0 - done_tensor) * self.gamma * next_q_tensor  # target网络 估计回报\r\n",
    "        critic_target_tensor = critic_target_tensor.detach()\r\n",
    "\r\n",
    "        state_action_tensor = torch.cat([state_tensor, action_tensor], 1)\r\n",
    "        critic_pred_tensor = self.critic_evaluate_net(state_action_tensor).squeeze(1)\r\n",
    "        critic_loss_tensor = self.critic_loss(critic_pred_tensor, critic_target_tensor)\r\n",
    "        self.critic_optimizer.zero_grad()\r\n",
    "        critic_loss_tensor.backward()\r\n",
    "        self.critic_optimizer.step()\r\n",
    "\r\n",
    "        # learn actor\r\n",
    "        pred_action_tensor = self.actor_evaluate_net(state_tensor)\r\n",
    "        pred_action_tensor = pred_action_tensor.clamp(self.action_low, self.action_high)\r\n",
    "        pred_state_action_tensor = torch.cat([state_tensor, pred_action_tensor], 1)\r\n",
    "        critic_pred_tensor = self.critic_evaluate_net(pred_state_action_tensor)\r\n",
    "        actor_loss_tensor = -critic_pred_tensor.mean()\r\n",
    "        self.actor_optimizer.zero_grad()\r\n",
    "        actor_loss_tensor.backward()\r\n",
    "        self.actor_optimizer.step()\r\n",
    "\r\n",
    "        self.update_net(self.critic_target_net, self.critic_evaluate_net)\r\n",
    "        self.update_net(self.actor_target_net, self.actor_evaluate_net)\r\n",
    "\r\n",
    "\r\n",
    "agent = DDPGAgent(env)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TD3Agent:\r\n",
    "    def __init__(self, env):\r\n",
    "        state_dim = env.observation_space.shape[0]\r\n",
    "        self.action_dim = env.action_space.shape[0]\r\n",
    "        self.action_low = env.action_space.low[0]\r\n",
    "        self.action_high = env.action_space.high[0]\r\n",
    "\r\n",
    "        self.gamma = 0.99\r\n",
    "\r\n",
    "        self.replayer = DQNReplayer(20000)\r\n",
    "\r\n",
    "        self.actor_evaluate_net = self.build_net(\r\n",
    "                input_size=state_dim, hidden_sizes=[32, 64],\r\n",
    "                output_size=self.action_dim)\r\n",
    "        self.actor_optimizer = optim.Adam(self.actor_evaluate_net.parameters(), lr=0.001)\r\n",
    "        self.actor_target_net = copy.deepcopy(self.actor_evaluate_net)\r\n",
    "\r\n",
    "        self.critic0_evaluate_net = self.build_net(\r\n",
    "                input_size=state_dim+self.action_dim, hidden_sizes=[64, 128])\r\n",
    "        self.critic0_optimizer = optim.Adam(self.critic0_evaluate_net.parameters(), lr=0.001)\r\n",
    "        self.critic0_loss = nn.MSELoss()\r\n",
    "        self.critic0_target_net = copy.deepcopy(self.critic0_evaluate_net)\r\n",
    "\r\n",
    "        self.critic1_evaluate_net = self.build_net(\r\n",
    "                input_size=state_dim+self.action_dim, hidden_sizes=[64, 128])\r\n",
    "        self.critic1_optimizer = optim.Adam(self.critic1_evaluate_net.parameters(), lr=0.001)\r\n",
    "        self.critic1_loss = nn.MSELoss()\r\n",
    "        self.critic1_target_net = copy.deepcopy(self.critic1_evaluate_net)\r\n",
    "\r\n",
    "    def build_net(self, input_size, hidden_sizes, output_size=1,\r\n",
    "            output_activator=None):\r\n",
    "        layers = []\r\n",
    "        for input_size, output_size in zip(\r\n",
    "                [input_size,] + hidden_sizes, hidden_sizes + [output_size,]):\r\n",
    "            layers.append(nn.Linear(input_size, output_size))\r\n",
    "            layers.append(nn.ReLU())\r\n",
    "        layers = layers[:-1]\r\n",
    "        if output_activator:\r\n",
    "            layers.append(output_activator)\r\n",
    "        net = nn.Sequential(*layers)\r\n",
    "        return net\r\n",
    "\r\n",
    "    def reset(self, mode=None):\r\n",
    "        self.mode = mode\r\n",
    "        if self.mode == 'train':\r\n",
    "            self.trajectory = []\r\n",
    "            self.noise = OrnsteinUhlenbeckProcess(np.zeros((self.action_dim,)))\r\n",
    "\r\n",
    "    def step(self, observation, reward, done):\r\n",
    "        state_tensor = torch.as_tensor(observation, dtype=torch.float).unsqueeze(0)\r\n",
    "        action_tensor = self.actor_evaluate_net(state_tensor)\r\n",
    "        action = action_tensor.detach().numpy()[0]\r\n",
    "\r\n",
    "        if self.mode == 'train':\r\n",
    "            # noisy action\r\n",
    "            action = (action + self.noise(sigma=0.1)).clip(self.action_low, self.action_high)\r\n",
    "\r\n",
    "            self.trajectory += [observation, reward, done, action]\r\n",
    "            if len(self.trajectory) >= 8:\r\n",
    "                state, _, _, act, next_state, reward, done, _ = self.trajectory[-8:]\r\n",
    "                self.replayer.store(state, act, reward, next_state, done)\r\n",
    "\r\n",
    "            # learn\r\n",
    "            if self.replayer.count >= 3000:\r\n",
    "                self.learn()\r\n",
    "        return action\r\n",
    "\r\n",
    "    def close(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def update_net(self, target_net, evaluate_net, learning_rate=0.005):\r\n",
    "        for target_param, evaluate_param in zip(\r\n",
    "                target_net.parameters(), evaluate_net.parameters()):\r\n",
    "            target_param.data.copy_(learning_rate * evaluate_param.data\r\n",
    "                    + (1 - learning_rate) * target_param.data)\r\n",
    "\r\n",
    "    def learn(self):\r\n",
    "        # replay\r\n",
    "        states, actions, rewards, next_states, dones = self.replayer.sample(64)\r\n",
    "        state_tensor = torch.as_tensor(states, dtype=torch.float)\r\n",
    "        action_tensor = torch.as_tensor(actions, dtype=torch.long)\r\n",
    "        reward_tensor = torch.as_tensor(rewards, dtype=torch.float)\r\n",
    "        next_state_tensor = torch.as_tensor(next_states, dtype=torch.float)\r\n",
    "        done_tensor = torch.as_tensor(dones, dtype=torch.float)\r\n",
    "\r\n",
    "        # learn critic\r\n",
    "        next_action_tensor = self.actor_target_net(next_state_tensor)\r\n",
    "        noise_tensor = (0.2 * torch.randn_like(action_tensor, dtype=torch.float))\r\n",
    "        noisy_next_action_tensor = (next_action_tensor + noise_tensor\r\n",
    "                    ).clamp(self.action_low, self.action_high)\r\n",
    "        next_state_action_tensor = torch.cat([next_state_tensor, noisy_next_action_tensor], 1)\r\n",
    "        next_q0_tensor = self.critic0_target_net(next_state_action_tensor).squeeze(1)\r\n",
    "        next_q1_tensor = self.critic1_target_net(next_state_action_tensor).squeeze(1)\r\n",
    "        next_q_tensor = torch.min(next_q0_tensor, next_q1_tensor)\r\n",
    "        critic_target_tensor = reward_tensor + (1. - done_tensor) * self.gamma * next_q_tensor\r\n",
    "        critic_target_tensor = critic_target_tensor.detach()\r\n",
    "\r\n",
    "        state_action_tensor = torch.cat([state_tensor, action_tensor], 1)\r\n",
    "        critic_pred0_tensor = self.critic0_evaluate_net(state_action_tensor).squeeze(1)\r\n",
    "        critic0_loss_tensor = self.critic0_loss(critic_pred0_tensor, critic_target_tensor)\r\n",
    "        self.critic0_optimizer.zero_grad()\r\n",
    "        critic0_loss_tensor.backward()\r\n",
    "        self.critic0_optimizer.step()\r\n",
    "\r\n",
    "        critic_pred1_tensor = self.critic1_evaluate_net(state_action_tensor).squeeze(1)\r\n",
    "        critic1_loss_tensor = self.critic1_loss(critic_pred1_tensor, critic_target_tensor)\r\n",
    "        self.critic1_optimizer.zero_grad()\r\n",
    "        critic1_loss_tensor.backward()\r\n",
    "        self.critic1_optimizer.step()\r\n",
    "\r\n",
    "        # learn actor\r\n",
    "        pred_action_tensor = self.actor_evaluate_net(state_tensor)\r\n",
    "        pred_action_tensor = pred_action_tensor.clamp(self.action_low, self.action_high)\r\n",
    "        pred_state_action_tensor = torch.cat([state_tensor, pred_action_tensor], 1)\r\n",
    "        critic_pred_tensor = self.critic0_evaluate_net(pred_state_action_tensor)\r\n",
    "        actor_loss_tensor = -critic_pred_tensor.mean()\r\n",
    "        self.actor_optimizer.zero_grad()\r\n",
    "        actor_loss_tensor.backward()\r\n",
    "        self.actor_optimizer.step()\r\n",
    "\r\n",
    "        self.update_net(self.critic0_target_net, self.critic0_evaluate_net)\r\n",
    "        self.update_net(self.critic1_target_net, self.critic1_evaluate_net)\r\n",
    "        self.update_net(self.actor_target_net, self.actor_evaluate_net)\r\n",
    "\r\n",
    "\r\n",
    "agent = TD3Agent(env)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def play_episode(env, agent, max_episode_steps=None, mode=None, render=False):\r\n",
    "    observation, reward, done = env.reset(), 0., False\r\n",
    "    agent.reset(mode=mode)\r\n",
    "    episode_reward, elapsed_steps = 0., 0\r\n",
    "    while True:\r\n",
    "        action = agent.step(observation, reward, done)\r\n",
    "        if render:\r\n",
    "            env.render()\r\n",
    "        if done:\r\n",
    "            break\r\n",
    "        observation, reward, done, _ = env.step(action)\r\n",
    "        episode_reward += reward\r\n",
    "        elapsed_steps += 1\r\n",
    "        if max_episode_steps and elapsed_steps >= max_episode_steps:\r\n",
    "            break\r\n",
    "    agent.close()\r\n",
    "    return episode_reward, elapsed_steps\r\n",
    "\r\n",
    "\r\n",
    "logging.info('==== train ====')\r\n",
    "episode_rewards = []\r\n",
    "for episode in itertools.count():\r\n",
    "    episode_reward, elapsed_steps = play_episode(env.unwrapped, agent,\r\n",
    "            max_episode_steps=env._max_episode_steps, mode='train')\r\n",
    "    episode_rewards.append(episode_reward)\r\n",
    "    logging.debug('train episode %d: reward = %.2f, steps = %d',\r\n",
    "            episode, episode_reward, elapsed_steps)\r\n",
    "    if np.mean(episode_rewards[-10:]) > -120:\r\n",
    "        break\r\n",
    "plt.plot(episode_rewards)\r\n",
    "\r\n",
    "\r\n",
    "logging.info('==== test ====')\r\n",
    "episode_rewards = []\r\n",
    "for episode in range(100):\r\n",
    "    episode_reward, elapsed_steps = play_episode(env, agent)\r\n",
    "    episode_rewards.append(episode_reward)\r\n",
    "    logging.debug('test episode %d: reward = %.2f, steps = %d',\r\n",
    "            episode, episode_reward, elapsed_steps)\r\n",
    "logging.info('average episode reward = %.2f ± %.2f',\r\n",
    "        np.mean(episode_rewards), np.std(episode_rewards))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('drl': conda)"
  },
  "interpreter": {
   "hash": "905757016efd07d0cfd81c6f9cd77a81e6638c87ef6a4e0f7fa650f59467cbb1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}