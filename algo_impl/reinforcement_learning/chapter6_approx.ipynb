{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%matplotlib inline\r\n",
    "\r\n",
    "import sys\r\n",
    "import logging\r\n",
    "import itertools\r\n",
    "import copy\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "np.random.seed(0)\r\n",
    "import pandas as pd\r\n",
    "import gym\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "torch.manual_seed(0)\r\n",
    "\r\n",
    "\r\n",
    "logging.basicConfig(format='%(asctime)s [%(levelname)s] %(message)s',\r\n",
    "                    datefmt='%Y/%m/%d %H:%M:%S',\r\n",
    "                    stream=sys.stdout,\r\n",
    "                    # filemode='w',\r\n",
    "                    # filename='log_{}.log'.format{time.strftime('%Y-%m-%d %H-%M-%S',time.localtime(time.time()))},\r\n",
    "                    level=logging.INFO)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "env = gym.make(\"MountainCar-v0\")\r\n",
    "env.seed(0)\r\n",
    "for key in vars(env):\r\n",
    "    logging.info(\"%s: %s\", key, vars(env)[key])\r\n",
    "print()\r\n",
    "for key in vars(env.spec):\r\n",
    "    logging.info(\"%s: %s\", key, vars(env.spec)[key])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021/08/11 19:56:30 [INFO] env: <MountainCarEnv<MountainCar-v0>>\n",
      "2021/08/11 19:56:30 [INFO] action_space: Discrete(3)\n",
      "2021/08/11 19:56:30 [INFO] observation_space: Box(-1.2000000476837158, 0.6000000238418579, (2,), float32)\n",
      "2021/08/11 19:56:30 [INFO] reward_range: (-inf, inf)\n",
      "2021/08/11 19:56:30 [INFO] metadata: {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 30}\n",
      "2021/08/11 19:56:30 [INFO] _max_episode_steps: 200\n",
      "2021/08/11 19:56:30 [INFO] _elapsed_steps: None\n",
      "\n",
      "2021/08/11 19:56:30 [INFO] id: MountainCar-v0\n",
      "2021/08/11 19:56:30 [INFO] entry_point: gym.envs.classic_control:MountainCarEnv\n",
      "2021/08/11 19:56:30 [INFO] reward_threshold: -110.0\n",
      "2021/08/11 19:56:30 [INFO] nondeterministic: False\n",
      "2021/08/11 19:56:30 [INFO] max_episode_steps: 200\n",
      "2021/08/11 19:56:30 [INFO] _kwargs: {}\n",
      "2021/08/11 19:56:30 [INFO] _env_name: MountainCar\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 6-1 导入小车上山环境\r\n",
    "\r\n",
    "# env = gym.make('MountainCar-v0')\r\n",
    "# env = env.unwrapped\r\n",
    "# print('观测空间 = {}'.format(env.observation_space))\r\n",
    "# print('动作空间 = {}'.format(env.action_space))\r\n",
    "# print('位置范围 = {}'.format(env.min_position, env.max_speed))\r\n",
    "# print('速度范围 = {}'.format((-env.max_speed, env.max_speed)))\r\n",
    "# print('目标位置 = {}'.format(env.goal_position))\r\n",
    "\r\n",
    "observation = env.reset()\r\n",
    "print(observation.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 6-2 总是向右施力的智能体\r\n",
    "\r\n",
    "positions, velocities = [], []\r\n",
    "observation = env.reset()\r\n",
    "i = 200\r\n",
    "while i > 0:\r\n",
    "    positions.append(observation[0])\r\n",
    "    velocities.append(observation[1])\r\n",
    "    next_observation, reward, done, _ = env.step(2)\r\n",
    "    if done:\r\n",
    "        break\r\n",
    "    observation = next_observation\r\n",
    "    i = i - 1 \r\n",
    "\r\n",
    "if next_observation[0] > 0.5:\r\n",
    "    print('成功到达')\r\n",
    "else:\r\n",
    "    print('失败退出')\r\n",
    "\r\n",
    "# 绘制位置和速度图像\r\n",
    "fig, ax = plt.subplots()\r\n",
    "ax.plot(positions, label='position')\r\n",
    "ax.plot(velocities, label='velocity')\r\n",
    "ax.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 6-3 砖瓦编码的实现\r\n",
    "\r\n",
    "class TileCoder:\r\n",
    "    def __init__(self, layers, features) -> None:\r\n",
    "        self.layers = layers\r\n",
    "        self.features = features\r\n",
    "        self.codebook = {}\r\n",
    "\r\n",
    "    def get_feature(self, codeword):\r\n",
    "        if codeword in self.codebook:\r\n",
    "            return self.codebook[codeword]\r\n",
    "        count = len(self.codebook)\r\n",
    "        if count >= self.features: # 冲突处理\r\n",
    "            return hash(codeword) & self.features\r\n",
    "        else:\r\n",
    "            self.codebook[codeword] = count\r\n",
    "            return count\r\n",
    "    \r\n",
    "    def __call__(self, floats=(), ints=()):\r\n",
    "        dim = len(floats)\r\n",
    "        scaled_floats = tuple(f * self.layers * self.layers for f in floats)\r\n",
    "        features = []\r\n",
    "        for layer in range(self.layers):\r\n",
    "            codeword = (layer,) + tuple(int((f + (1 + dim * i) * layer) / self.layers) for i, f in enumerate(scaled_floats)) + ints\r\n",
    "            feature = self.get_feature(codeword)\r\n",
    "            features.append(feature)\r\n",
    "        return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 6-4 函数近似SARSA算法智能体"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 6-5 函数近似SARSA(λ)智能体"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# 6-6 经验回访的实现\r\n",
    "\r\n",
    "class DQNReplayer:\r\n",
    "    def __init__(self, capacity) -> None:\r\n",
    "        self.memory = pd.DataFrame(\r\n",
    "            index=range(capacity), columns=[\"observation\", \"action\", \"reward\", \"next_observation\", \"done\"]\r\n",
    "        )  # memory存储(s, a, r, s')\r\n",
    "        self.i = 0  # 最新存储位置的索引\r\n",
    "        self.count = 0\r\n",
    "        self.capacity = capacity\r\n",
    "\r\n",
    "    def store(self, *args):\r\n",
    "        self.memory.loc[self.i] = args\r\n",
    "        self.i = (self.i + 1) % self.capacity\r\n",
    "        self.count = min(self.count + 1, self.capacity)\r\n",
    "\r\n",
    "    def sample(self, size):\r\n",
    "        indices = np.random.choice(self.count, size=size)\r\n",
    "        return (np.stack(self.memory.loc[indices, field]) for field in self.memory.columns)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 6-7 带目标网络的深度Q学习智能体\r\n",
    "\r\n",
    "class DQNAgent:\r\n",
    "    def __init__(self, env) -> None:\r\n",
    "        self.action_n = env.action_space.n\r\n",
    "        self.gamma = 0.99\r\n",
    "        self.replayer = DQNReplayer(10000)\r\n",
    "        self.evaluate_net = self.build_net(\r\n",
    "            input_size=env.observation_space.shape[0], hidden_sizes=[64, 64], output_size=self.action_n\r\n",
    "        )\r\n",
    "        self.optimizer = optim.Adam(self.evaluate_net.parameters(), lr=0.001)\r\n",
    "        self.loss = nn.MSELoss()\r\n",
    "\r\n",
    "    def build_net(self, input_size, hidden_sizes, output_size):\r\n",
    "        layers = []\r\n",
    "        for input_size, output_size in zip(\r\n",
    "            [\r\n",
    "                input_size,\r\n",
    "            ]\r\n",
    "            + hidden_sizes,\r\n",
    "            hidden_sizes\r\n",
    "            + [\r\n",
    "                output_size,\r\n",
    "            ],\r\n",
    "        ):\r\n",
    "            layers.append(nn.Linear(input_size, output_size))\r\n",
    "            layers.append(nn.ReLU())\r\n",
    "        layers = layers[:-1]\r\n",
    "        model = nn.Sequential(*layers)\r\n",
    "        print(model)\r\n",
    "        return model\r\n",
    "\r\n",
    "    def reset(self, mode=None):\r\n",
    "        self.mode = mode\r\n",
    "        if self.mode == \"train\":\r\n",
    "            self.trajectory = []\r\n",
    "            self.target_net = copy.deepcopy(self.evaluate_net)\r\n",
    "\r\n",
    "    def step(self, observation, reward, done):\r\n",
    "        # 根据状态和奖励选择下一个动作\r\n",
    "        if self.mode == \"train\" and np.random.rand() < 0.001:\r\n",
    "            # epsilon-greedy policy in train mode\r\n",
    "            action = np.random.randint(self.action_n)\r\n",
    "        else:\r\n",
    "            # test\r\n",
    "            state_tensor = torch.as_tensor(observation, dtype=torch.float).squeeze(0)\r\n",
    "            q_tensor = self.evaluate_net(state_tensor)\r\n",
    "            action_tensor = torch.argmax(q_tensor)\r\n",
    "            action = action_tensor.item()\r\n",
    "\r\n",
    "        if self.mode == \"train\":\r\n",
    "            self.trajectory += [observation, reward, done, action]\r\n",
    "            if len(self.trajectory) >= 8:\r\n",
    "                state, _, _, act, next_state, reward, done, _ = self.trajectory[-8:]\r\n",
    "                self.replayer.store(state, act, reward, next_state, done)\r\n",
    "\r\n",
    "            if self.replayer.count > self.replayer.capacity * 0.95:  # skip first few episodes for speed\r\n",
    "                self.learn()\r\n",
    "\r\n",
    "        return action\r\n",
    "\r\n",
    "    def learn(self):\r\n",
    "        # replay\r\n",
    "        states, actions, rewards, next_states, dones = self.replayer.sample(1024)\r\n",
    "        state_tensor = torch.as_tensor(states, dtype=torch.float)\r\n",
    "        action_tensor = torch.as_tensor(actions, dtype=torch.long)\r\n",
    "        reward_tensor = torch.as_tensor(rewards, dtype=torch.float)\r\n",
    "        next_state_tensor = torch.as_tensor(next_states, dtype=torch.float)\r\n",
    "        done_tensor = torch.as_tensor(dones, dtype=torch.float)\r\n",
    "\r\n",
    "        # train\r\n",
    "        next_q_tensor = self.target_net(next_state_tensor)\r\n",
    "        next_max_q_tensor, _ = next_q_tensor.max(axis=-1)  # q_learning\r\n",
    "        target_tensor = reward_tensor + self.gamma * (1.0 - done_tensor) * next_max_q_tensor\r\n",
    "        pred_tensor = self.evaluate_net(state_tensor)\r\n",
    "        q_tensor = pred_tensor.gather(1, action_tensor.unsqueeze(1)).squeeze(1)\r\n",
    "        loss_tensor = self.loss(target_tensor, q_tensor)\r\n",
    "        self.optimizer.zero_grad()\r\n",
    "        loss_tensor.backward()\r\n",
    "        self.optimizer.step()\r\n",
    "\r\n",
    "    def close(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "\r\n",
    "agent = DQNAgent(env)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# DoubleDQN\r\n",
    "\r\n",
    "class DoubleDQNAgent:\r\n",
    "    def __init__(self, env) -> None:\r\n",
    "        self.action_n = env.action_space.n\r\n",
    "        self.gamma = 0.99\r\n",
    "        self.replayer = DQNReplayer(10000)\r\n",
    "        self.evaluate_net = self.build_net(\r\n",
    "            input_size=env.observation_space.shape[0], hidden_sizes=[64, 64], output_size=self.action_n\r\n",
    "        )\r\n",
    "        self.optimizer = optim.Adam(self.evaluate_net.parameters(), lr=0.001)\r\n",
    "        self.loss = nn.MSELoss()\r\n",
    "\r\n",
    "    def build_net(self, input_size, hidden_sizes, output_size):\r\n",
    "        layers = []\r\n",
    "        for input_size, output_size in zip(\r\n",
    "            [\r\n",
    "                input_size,\r\n",
    "            ]\r\n",
    "            + hidden_sizes,\r\n",
    "            hidden_sizes\r\n",
    "            + [\r\n",
    "                output_size,\r\n",
    "            ],\r\n",
    "        ):\r\n",
    "            layers.append(nn.Linear(input_size, output_size))\r\n",
    "            layers.append(nn.ReLU())\r\n",
    "        layers = layers[:-1]\r\n",
    "        model = nn.Sequential(*layers)\r\n",
    "        print(model)\r\n",
    "        return model\r\n",
    "    \r\n",
    "    def reset(self, mode=None):\r\n",
    "        self.mode = mode\r\n",
    "        if self.mode == 'train':\r\n",
    "            self.trajectory = []\r\n",
    "            self.target_net = copy.deepcopy(self.evaluate_net)\r\n",
    "\r\n",
    "    def step(self, observation, reward, done):\r\n",
    "        if self.mode == 'train' and np.random.rand() < 0.001:\r\n",
    "            # epsilon-greedy policy in train mode\r\n",
    "            action = np.random.randint(self.action_n)\r\n",
    "        else:\r\n",
    "            state_tensor = torch.as_tensor(observation,\r\n",
    "                    dtype=torch.float).reshape(1, -1)\r\n",
    "            q_tensor = self.evaluate_net(state_tensor)\r\n",
    "            action_tensor = torch.argmax(q_tensor)\r\n",
    "            action = action_tensor.item()\r\n",
    "        if self.mode == 'train':\r\n",
    "            self.trajectory += [observation, reward, done, action]\r\n",
    "            if len(self.trajectory) >= 8:\r\n",
    "                state, _, _, act, next_state, reward, done, _ = \\\r\n",
    "                        self.trajectory[-8:]\r\n",
    "                self.replayer.store(state, act, reward, next_state, done)\r\n",
    "            if self.replayer.count >= self.replayer.capacity * 0.95:\r\n",
    "                    # skip first few episodes for speed\r\n",
    "                self.learn()\r\n",
    "        return action\r\n",
    "\r\n",
    "    def close(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def learn(self):\r\n",
    "        # replay\r\n",
    "        states, actions, rewards, next_states, dones = \\\r\n",
    "                self.replayer.sample(1024) # replay transitions\r\n",
    "        state_tensor = torch.as_tensor(states, dtype=torch.float)\r\n",
    "        action_tensor = torch.as_tensor(actions, dtype=torch.long)\r\n",
    "        reward_tensor = torch.as_tensor(rewards, dtype=torch.float)\r\n",
    "        next_state_tensor = torch.as_tensor(next_states, dtype=torch.float)\r\n",
    "        done_tensor = torch.as_tensor(dones, dtype=torch.float)\r\n",
    "\r\n",
    "        # train\r\n",
    "        next_eval_q_tensor = self.evaluate_net(next_state_tensor)\r\n",
    "        next_action_tensor = next_eval_q_tensor.argmax(axis=-1)\r\n",
    "        next_q_tensor = self.target_net(next_state_tensor)\r\n",
    "        next_max_q_tensor = torch.gather(next_q_tensor, 1,\r\n",
    "                next_action_tensor.unsqueeze(1)).squeeze(1)\r\n",
    "        target_tensor = reward_tensor + self.gamma * (1. - done_tensor) * next_max_q_tensor\r\n",
    "        pred_tensor = self.evaluate_net(state_tensor)\r\n",
    "        q_tensor = pred_tensor.gather(1, action_tensor.unsqueeze(1)).squeeze(1)\r\n",
    "        loss_tensor = self.loss(target_tensor, q_tensor)\r\n",
    "        self.optimizer.zero_grad()\r\n",
    "        loss_tensor.backward()\r\n",
    "        self.optimizer.step()\r\n",
    "\r\n",
    "agent = DoubleDQNAgent(env)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# DuelDQN\r\n",
    "\r\n",
    "class DuelNet(nn.Module):\r\n",
    "    def __init__(self, input_size, output_size):\r\n",
    "        super().__init__()\r\n",
    "        self.common_net = nn.Sequential(nn.Linear(input_size, 64), nn.ReLU())\r\n",
    "        self.advantage_net = nn.Sequential(nn.Linear(64, 32), nn.ReLU(),\r\n",
    "            nn.Linear(32, output_size))\r\n",
    "        self.v_net = nn.Sequential(nn.Linear(64, 32), nn.ReLU(), nn.Linear(32, 1))\r\n",
    "\r\n",
    "    def forward(self, s):\r\n",
    "        h = self.common_net(s)\r\n",
    "        adv = self.advantage_net(h)\r\n",
    "        adv = adv - adv.mean(1).unsqueeze(1)\r\n",
    "        v = self.v_net(h)\r\n",
    "        q = v + adv\r\n",
    "        return q\r\n",
    "\r\n",
    "class DuelDQNAgent:\r\n",
    "    def __init__(self, env):\r\n",
    "        self.action_n = env.action_space.n\r\n",
    "        self.gamma = 0.99\r\n",
    "        self.replayer = DQNReplayer(10000)\r\n",
    "        self.evaluate_net = DuelNet(input_size=env.observation_space.shape[0],\r\n",
    "                output_size=self.action_n)\r\n",
    "        self.optimizer = optim.Adam(self.evaluate_net.parameters(), lr=0.001)\r\n",
    "        self.loss = nn.MSELoss()\r\n",
    "\r\n",
    "    def reset(self, mode=None):\r\n",
    "        self.mode = mode\r\n",
    "        if self.mode == 'train':\r\n",
    "            self.trajectory = []\r\n",
    "            self.target_net = copy.deepcopy(self.evaluate_net)\r\n",
    "\r\n",
    "    def step(self, observation, reward, done):\r\n",
    "        if self.mode == 'train' and np.random.rand() < 0.001:\r\n",
    "            # epsilon-greedy policy in train mode\r\n",
    "            action = np.random.randint(self.action_n)\r\n",
    "        else:\r\n",
    "            state_tensor = torch.as_tensor(observation,\r\n",
    "                    dtype=torch.float).reshape(1, -1)\r\n",
    "            q_tensor = self.evaluate_net(state_tensor)\r\n",
    "            action_tensor = torch.argmax(q_tensor)\r\n",
    "            action = action_tensor.item()\r\n",
    "        if self.mode == 'train':\r\n",
    "            self.trajectory += [observation, reward, done, action]\r\n",
    "            if len(self.trajectory) >= 8:\r\n",
    "                state, _, _, act, next_state, reward, done, _ = \\\r\n",
    "                        self.trajectory[-8:]\r\n",
    "                self.replayer.store(state, act, reward, next_state, done)\r\n",
    "            if self.replayer.count >= self.replayer.capacity * 0.95:\r\n",
    "                    # skip first few episodes for speed\r\n",
    "                self.learn()\r\n",
    "        return action\r\n",
    "\r\n",
    "    def close(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def learn(self):\r\n",
    "        # replay\r\n",
    "        states, actions, rewards, next_states, dones = \\\r\n",
    "                self.replayer.sample(1024) # replay transitions\r\n",
    "        state_tensor = torch.as_tensor(states, dtype=torch.float)\r\n",
    "        action_tensor = torch.as_tensor(actions, dtype=torch.long)\r\n",
    "        reward_tensor = torch.as_tensor(rewards, dtype=torch.float)\r\n",
    "        next_state_tensor = torch.as_tensor(next_states, dtype=torch.float)\r\n",
    "        done_tensor = torch.as_tensor(dones, dtype=torch.float)\r\n",
    "\r\n",
    "        # train\r\n",
    "        next_eval_q_tensor = self.evaluate_net(next_state_tensor)\r\n",
    "        next_action_tensor = next_eval_q_tensor.argmax(axis=-1)\r\n",
    "        next_q_tensor = self.target_net(next_state_tensor)\r\n",
    "        next_max_q_tensor = torch.gather(next_q_tensor, 1,\r\n",
    "                next_action_tensor.unsqueeze(1)).squeeze(1)\r\n",
    "        target_tensor = reward_tensor + self.gamma * (1. - done_tensor) * \\\r\n",
    "                next_max_q_tensor\r\n",
    "        pred_tensor = self.evaluate_net(state_tensor)\r\n",
    "        unsqueeze_tensor = action_tensor.unsqueeze(1)\r\n",
    "        q_tensor = pred_tensor.gather(1, action_tensor.unsqueeze(1)).squeeze(1)\r\n",
    "        loss_tensor = self.loss(target_tensor, q_tensor)\r\n",
    "        self.optimizer.zero_grad()\r\n",
    "        loss_tensor.backward()\r\n",
    "        self.optimizer.step()\r\n",
    "\r\n",
    "\r\n",
    "agent = DuelDQNAgent(env)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def play_episode(env, agent, max_episode_steps=None, mode=None, render=False):\r\n",
    "    observation, reward, done = env.reset(), 0.0, False\r\n",
    "    agent.reset(mode=mode)\r\n",
    "    episode_reward, elapsed_steps = 0.0, 0\r\n",
    "    while True:\r\n",
    "        action = agent.step(observation, reward, done)\r\n",
    "        if render:\r\n",
    "            env.render()\r\n",
    "        if done:\r\n",
    "            break\r\n",
    "        observation, reward, done, _ = env.step(action)\r\n",
    "        episode_reward += reward\r\n",
    "        elapsed_steps += 1\r\n",
    "        if max_episode_steps and elapsed_steps >= max_episode_steps:\r\n",
    "            break\r\n",
    "    agent.close()\r\n",
    "    return episode_reward, elapsed_steps\r\n",
    "\r\n",
    "\r\n",
    "logging.info(\"==== train ====\")\r\n",
    "episode_rewards = []\r\n",
    "for episode in itertools.count():\r\n",
    "    episode_reward, elapsed_steps = play_episode(\r\n",
    "        env.unwrapped, agent, max_episode_steps=env._max_episode_steps, mode=\"train\", render=False\r\n",
    "    )\r\n",
    "    episode_rewards.append(episode_reward)\r\n",
    "    logging.debug(\"train episode %d: reward = %.2f, steps = %d\", episode, episode_reward, elapsed_steps)\r\n",
    "    if np.mean(episode_rewards[-10:]) > -110:\r\n",
    "        break\r\n",
    "plt.plot(episode_rewards)\r\n",
    "\r\n",
    "\r\n",
    "logging.info(\"==== test ====\")\r\n",
    "episode_rewards = []\r\n",
    "for episode in range(100):\r\n",
    "    episode_reward, elapsed_steps = play_episode(env, agent)\r\n",
    "    episode_rewards.append(episode_reward)\r\n",
    "    logging.debug(\"test episode %d: reward = %.2f, steps = %d\", episode, episode_reward, elapsed_steps)\r\n",
    "logging.info(\"average episode reward = %.2f ± %.2f\", np.mean(episode_rewards), np.std(episode_rewards))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021/08/11 20:14:29 [INFO] ==== train ====\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ef6b4ba8b348>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mepisode_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     episode_reward, elapsed_steps = play_episode(\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_episode_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     )\n",
      "\u001b[1;32m<ipython-input-7-ef6b4ba8b348>\u001b[0m in \u001b[0;36mplay_episode\u001b[1;34m(env, agent, max_episode_steps, mode, render)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mepisode_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melapsed_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-262feda8b08e>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, observation, reward, done)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapacity\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.95\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                     \u001b[1;31m# skip first few episodes for speed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-262feda8b08e>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mloss_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mloss_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "af43dd1a3aed37f0269457c8a9208500fedd5a39c3ade42feceda1b56cea2745"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}